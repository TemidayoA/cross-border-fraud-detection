{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Feature Engineering\n",
    "\n",
    "This notebook implements corridor-normalised features that form the foundation of our context-aware fraud detection system.\n",
    "\n",
    "## Objectives\n",
    "1. Build corridor-specific statistical profiles\n",
    "2. Implement normalised feature calculations\n",
    "3. Demonstrate how normalisation improves signal quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Load data from previous notebook\n",
    "transactions = pd.read_csv('synthetic_transactions.csv', parse_dates=['timestamp'])\n",
    "print(f'Loaded {len(transactions):,} transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Corridor Statistical Profiles\n",
    "\n",
    "Each corridor needs a statistical profile that captures \"normal\" behaviour for that specific payment route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorridorProfiler:\n",
    "    \"\"\"\n",
    "    Builds and stores statistical profiles for each payment corridor.\n",
    "    These profiles define what 'normal' looks like for each route.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.profiles = {}\n",
    "    \n",
    "    def fit(self, transactions_df):\n",
    "        \"\"\"\n",
    "        Calculate corridor profiles from historical transaction data.\n",
    "        In production, this would be run on fraud-free (or low-fraud) historical data.\n",
    "        \"\"\"\n",
    "        for corridor in transactions_df['corridor'].unique():\n",
    "            corridor_data = transactions_df[transactions_df['corridor'] == corridor]\n",
    "            \n",
    "            # Amount statistics\n",
    "            amount_stats = {\n",
    "                'median': corridor_data['amount'].median(),\n",
    "                'mean': corridor_data['amount'].mean(),\n",
    "                'std': corridor_data['amount'].std(),\n",
    "                'p25': corridor_data['amount'].quantile(0.25),\n",
    "                'p75': corridor_data['amount'].quantile(0.75),\n",
    "                'p90': corridor_data['amount'].quantile(0.90),\n",
    "                'p95': corridor_data['amount'].quantile(0.95),\n",
    "                'p99': corridor_data['amount'].quantile(0.99),\n",
    "            }\n",
    "            \n",
    "            # Velocity statistics (transactions per sender)\n",
    "            sender_counts = corridor_data.groupby('sender_id').size()\n",
    "            velocity_stats = {\n",
    "                'median_txn_per_sender': sender_counts.median(),\n",
    "                'mean_txn_per_sender': sender_counts.mean(),\n",
    "                'p95_txn_per_sender': sender_counts.quantile(0.95),\n",
    "            }\n",
    "            \n",
    "            # Temporal patterns\n",
    "            corridor_data = corridor_data.copy()\n",
    "            corridor_data['hour'] = corridor_data['timestamp'].dt.hour\n",
    "            corridor_data['dayofweek'] = corridor_data['timestamp'].dt.dayofweek\n",
    "            \n",
    "            hour_dist = corridor_data['hour'].value_counts(normalize=True).to_dict()\n",
    "            day_dist = corridor_data['dayofweek'].value_counts(normalize=True).to_dict()\n",
    "            \n",
    "            # Peak hours (top 5 hours by volume)\n",
    "            peak_hours = corridor_data['hour'].value_counts().head(5).index.tolist()\n",
    "            \n",
    "            temporal_stats = {\n",
    "                'hour_distribution': hour_dist,\n",
    "                'day_distribution': day_dist,\n",
    "                'peak_hours': peak_hours,\n",
    "            }\n",
    "            \n",
    "            self.profiles[corridor] = {\n",
    "                'corridor': corridor,\n",
    "                'corridor_name': corridor_data['corridor_name'].iloc[0],\n",
    "                'n_transactions': len(corridor_data),\n",
    "                'n_unique_senders': corridor_data['sender_id'].nunique(),\n",
    "                'amount': amount_stats,\n",
    "                'velocity': velocity_stats,\n",
    "                'temporal': temporal_stats,\n",
    "            }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_profile(self, corridor):\n",
    "        \"\"\"Retrieve profile for a specific corridor.\"\"\"\n",
    "        return self.profiles.get(corridor)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print summary of all corridor profiles.\"\"\"\n",
    "        rows = []\n",
    "        for corridor, profile in self.profiles.items():\n",
    "            rows.append({\n",
    "                'Corridor': profile['corridor_name'],\n",
    "                'Transactions': profile['n_transactions'],\n",
    "                'Unique Senders': profile['n_unique_senders'],\n",
    "                'Median Amount': f\"£{profile['amount']['median']:.0f}\",\n",
    "                '95th %ile Amount': f\"£{profile['amount']['p95']:.0f}\",\n",
    "                'Avg Txn/Sender': f\"{profile['velocity']['mean_txn_per_sender']:.1f}\",\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# Build profiles\n",
    "profiler = CorridorProfiler()\n",
    "profiler.fit(transactions)\n",
    "\n",
    "print('=== Corridor Profiles ===')\n",
    "print(profiler.summary().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Corridor-Normalised Features\n",
    "\n",
    "Now we build features that are meaningful within the context of each corridor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorridorFeatureEngine:\n",
    "    \"\"\"\n",
    "    Calculates corridor-normalised features for fraud detection.\n",
    "    Each feature is calibrated against corridor-specific baselines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, profiler):\n",
    "        self.profiler = profiler\n",
    "        self.sender_history = defaultdict(lambda: {\n",
    "            'transactions': [],\n",
    "            'beneficiaries': set(),\n",
    "            'devices': set(),\n",
    "            'first_seen': None,\n",
    "        })\n",
    "    \n",
    "    def _build_sender_history(self, transactions_df):\n",
    "        \"\"\"Build sender transaction history for velocity calculations.\"\"\"\n",
    "        sorted_txns = transactions_df.sort_values('timestamp')\n",
    "        \n",
    "        for _, row in sorted_txns.iterrows():\n",
    "            sender = row['sender_id']\n",
    "            self.sender_history[sender]['transactions'].append({\n",
    "                'timestamp': row['timestamp'],\n",
    "                'amount': row['amount'],\n",
    "                'corridor': row['corridor'],\n",
    "            })\n",
    "            if self.sender_history[sender]['first_seen'] is None:\n",
    "                self.sender_history[sender]['first_seen'] = row['timestamp']\n",
    "    \n",
    "    def amount_deviation_score(self, amount, corridor):\n",
    "        \"\"\"\n",
    "        Calculate how unusual the amount is for this corridor.\n",
    "        Returns 0-1 score where higher = more unusual.\n",
    "        \"\"\"\n",
    "        profile = self.profiler.get_profile(corridor)\n",
    "        if not profile:\n",
    "            return 0.5  # Default for unknown corridors\n",
    "        \n",
    "        median = profile['amount']['median']\n",
    "        p95 = profile['amount']['p95']\n",
    "        p99 = profile['amount']['p99']\n",
    "        \n",
    "        if amount <= median:\n",
    "            return 0.0\n",
    "        elif amount <= p95:\n",
    "            # Linear scale from 0 to 0.5 between median and p95\n",
    "            return ((amount - median) / (p95 - median)) * 0.5\n",
    "        elif amount <= p99:\n",
    "            # Linear scale from 0.5 to 0.8 between p95 and p99\n",
    "            return 0.5 + ((amount - p95) / (p99 - p95)) * 0.3\n",
    "        else:\n",
    "            # Above p99: scale towards 1.0 but cap\n",
    "            excess = (amount - p99) / p99\n",
    "            return min(0.8 + excess * 0.2, 1.0)\n",
    "    \n",
    "    def velocity_score(self, sender_id, current_timestamp, corridor, window_hours=24):\n",
    "        \"\"\"\n",
    "        Calculate normalised transaction velocity.\n",
    "        Compares sender's recent activity to corridor norms.\n",
    "        \"\"\"\n",
    "        profile = self.profiler.get_profile(corridor)\n",
    "        if not profile:\n",
    "            return 0.5\n",
    "        \n",
    "        history = self.sender_history.get(sender_id, {'transactions': []})\n",
    "        \n",
    "        # Count transactions in window\n",
    "        window_start = current_timestamp - timedelta(hours=window_hours)\n",
    "        recent_count = sum(\n",
    "            1 for txn in history['transactions']\n",
    "            if window_start <= txn['timestamp'] < current_timestamp\n",
    "        )\n",
    "        \n",
    "        # Normalise against corridor median\n",
    "        corridor_median = profile['velocity']['median_txn_per_sender']\n",
    "        corridor_p95 = profile['velocity']['p95_txn_per_sender']\n",
    "        \n",
    "        # Scale: daily velocity vs expected monthly/period velocity\n",
    "        # Simplified: compare count to what's normal for a day\n",
    "        expected_daily = corridor_median / 30  # Rough daily expectation\n",
    "        \n",
    "        if recent_count <= expected_daily:\n",
    "            return 0.0\n",
    "        elif recent_count <= expected_daily * 3:\n",
    "            return 0.3\n",
    "        elif recent_count <= expected_daily * 5:\n",
    "            return 0.6\n",
    "        else:\n",
    "            return min(0.6 + (recent_count - expected_daily * 5) * 0.1, 1.0)\n",
    "    \n",
    "    def temporal_anomaly_score(self, timestamp, corridor):\n",
    "        \"\"\"\n",
    "        Score how unusual the transaction timing is for this corridor.\n",
    "        \"\"\"\n",
    "        profile = self.profiler.get_profile(corridor)\n",
    "        if not profile:\n",
    "            return 0.0\n",
    "        \n",
    "        hour = timestamp.hour\n",
    "        peak_hours = profile['temporal']['peak_hours']\n",
    "        hour_dist = profile['temporal']['hour_distribution']\n",
    "        \n",
    "        # Check if hour is in peak hours\n",
    "        if hour in peak_hours:\n",
    "            return 0.0\n",
    "        \n",
    "        # Check how common this hour is\n",
    "        hour_probability = hour_dist.get(hour, 0.01)\n",
    "        \n",
    "        if hour_probability > 0.05:  # >5% of transactions\n",
    "            return 0.1\n",
    "        elif hour_probability > 0.02:\n",
    "            return 0.3\n",
    "        else:\n",
    "            return 0.5\n",
    "    \n",
    "    def sender_maturity_score(self, sender_id, current_timestamp):\n",
    "        \"\"\"\n",
    "        Score based on how established the sender is.\n",
    "        New accounts are higher risk.\n",
    "        \"\"\"\n",
    "        history = self.sender_history.get(sender_id)\n",
    "        \n",
    "        if not history or not history['first_seen']:\n",
    "            return 0.8  # New sender = elevated risk\n",
    "        \n",
    "        account_age_days = (current_timestamp - history['first_seen']).days\n",
    "        n_transactions = len(history['transactions'])\n",
    "        \n",
    "        if account_age_days < 7:\n",
    "            return 0.7\n",
    "        elif account_age_days < 30:\n",
    "            return 0.4\n",
    "        elif account_age_days < 90:\n",
    "            return 0.2\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_features(self, transaction):\n",
    "        \"\"\"\n",
    "        Calculate all corridor-normalised features for a transaction.\n",
    "        Returns dictionary of feature scores (all 0-1 range).\n",
    "        \"\"\"\n",
    "        corridor = transaction['corridor']\n",
    "        \n",
    "        features = {\n",
    "            'amount_deviation': self.amount_deviation_score(\n",
    "                transaction['amount'], corridor\n",
    "            ),\n",
    "            'velocity': self.velocity_score(\n",
    "                transaction['sender_id'],\n",
    "                transaction['timestamp'],\n",
    "                corridor\n",
    "            ),\n",
    "            'temporal_anomaly': self.temporal_anomaly_score(\n",
    "                transaction['timestamp'], corridor\n",
    "            ),\n",
    "            'sender_maturity': self.sender_maturity_score(\n",
    "                transaction['sender_id'],\n",
    "                transaction['timestamp']\n",
    "            ),\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Initialise feature engine\n",
    "feature_engine = CorridorFeatureEngine(profiler)\n",
    "feature_engine._build_sender_history(transactions)\n",
    "\n",
    "print('Feature engine initialised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features for all transactions\n",
    "print('Calculating features for all transactions...')\n",
    "\n",
    "feature_records = []\n",
    "for idx, row in transactions.iterrows():\n",
    "    features = feature_engine.calculate_features(row)\n",
    "    features['transaction_idx'] = idx\n",
    "    features['corridor'] = row['corridor']\n",
    "    features['corridor_name'] = row['corridor_name']\n",
    "    features['is_fraud'] = row['is_fraud']\n",
    "    features['amount'] = row['amount']\n",
    "    feature_records.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(feature_records)\n",
    "print(f'Calculated features for {len(features_df):,} transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse Feature Distributions\n",
    "\n",
    "Let's see how our normalised features compare between fraud and legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature distributions: Fraud vs Legitimate\n",
    "feature_cols = ['amount_deviation', 'velocity', 'temporal_anomaly', 'sender_maturity']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Separate fraud and legitimate\n",
    "    fraud_values = features_df[features_df['is_fraud']][feature]\n",
    "    legit_values = features_df[~features_df['is_fraud']][feature]\n",
    "    \n",
    "    ax.hist(legit_values, bins=30, alpha=0.6, label='Legitimate', density=True)\n",
    "    ax.hist(fraud_values, bins=30, alpha=0.6, label='Fraud', density=True)\n",
    "    \n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()} Score Distribution')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Feature Score Distributions: Fraud vs Legitimate', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics by fraud status\n",
    "print('=== Feature Statistics by Fraud Status ===\\n')\n",
    "\n",
    "for feature in feature_cols:\n",
    "    fraud_mean = features_df[features_df['is_fraud']][feature].mean()\n",
    "    legit_mean = features_df[~features_df['is_fraud']][feature].mean()\n",
    "    separation = fraud_mean - legit_mean\n",
    "    \n",
    "    print(f'{feature}:')\n",
    "    print(f'  Legitimate mean: {legit_mean:.3f}')\n",
    "    print(f'  Fraud mean:      {fraud_mean:.3f}')\n",
    "    print(f'  Separation:      {separation:+.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare: Global vs Corridor-Normalised Features\n",
    "\n",
    "Let's demonstrate the improvement from corridor normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_amount_score(amount, all_transactions):\n",
    "    \"\"\"\n",
    "    Calculate amount score using global (not corridor-specific) statistics.\n",
    "    This is the 'naive' approach.\n",
    "    \"\"\"\n",
    "    global_median = all_transactions['amount'].median()\n",
    "    global_p95 = all_transactions['amount'].quantile(0.95)\n",
    "    global_p99 = all_transactions['amount'].quantile(0.99)\n",
    "    \n",
    "    if amount <= global_median:\n",
    "        return 0.0\n",
    "    elif amount <= global_p95:\n",
    "        return ((amount - global_median) / (global_p95 - global_median)) * 0.5\n",
    "    elif amount <= global_p99:\n",
    "        return 0.5 + ((amount - global_p95) / (global_p99 - global_p95)) * 0.3\n",
    "    else:\n",
    "        excess = (amount - global_p99) / global_p99\n",
    "        return min(0.8 + excess * 0.2, 1.0)\n",
    "\n",
    "# Calculate global (non-normalised) amount scores\n",
    "features_df['amount_deviation_global'] = features_df['amount'].apply(\n",
    "    lambda x: global_amount_score(x, transactions)\n",
    ")\n",
    "\n",
    "print('Calculated global amount scores for comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance: Global vs Corridor-Normalised\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('=== Feature Quality Comparison (ROC-AUC) ===\\n')\n",
    "print('Higher AUC = better separation between fraud and legitimate\\n')\n",
    "\n",
    "# Overall comparison\n",
    "global_auc = roc_auc_score(features_df['is_fraud'], features_df['amount_deviation_global'])\n",
    "normalised_auc = roc_auc_score(features_df['is_fraud'], features_df['amount_deviation'])\n",
    "\n",
    "print(f'Overall (all corridors):')\n",
    "print(f'  Global amount score AUC:     {global_auc:.4f}')\n",
    "print(f'  Normalised amount score AUC: {normalised_auc:.4f}')\n",
    "print(f'  Improvement:                 {(normalised_auc - global_auc) * 100:+.2f}%\\n')\n",
    "\n",
    "# Per-corridor comparison\n",
    "print('Per-corridor breakdown:')\n",
    "for corridor_name in features_df['corridor_name'].unique():\n",
    "    corridor_data = features_df[features_df['corridor_name'] == corridor_name]\n",
    "    \n",
    "    if corridor_data['is_fraud'].sum() < 5:  # Need minimum fraud cases\n",
    "        continue\n",
    "    \n",
    "    global_auc = roc_auc_score(corridor_data['is_fraud'], corridor_data['amount_deviation_global'])\n",
    "    normalised_auc = roc_auc_score(corridor_data['is_fraud'], corridor_data['amount_deviation'])\n",
    "    \n",
    "    print(f'  {corridor_name}: Global={global_auc:.3f}, Normalised={normalised_auc:.3f} ({(normalised_auc-global_auc)*100:+.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the improvement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Global scores by corridor\n",
    "ax1 = axes[0]\n",
    "for corridor_name in features_df['corridor_name'].unique():\n",
    "    corridor_data = features_df[features_df['corridor_name'] == corridor_name]\n",
    "    fraud_scores = corridor_data[corridor_data['is_fraud']]['amount_deviation_global']\n",
    "    ax1.hist(fraud_scores, bins=20, alpha=0.5, label=corridor_name, density=True)\n",
    "\n",
    "ax1.set_title('Global Amount Scores (Fraud Only)\\nScores overlap significantly')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.legend()\n",
    "\n",
    "# Normalised scores by corridor\n",
    "ax2 = axes[1]\n",
    "for corridor_name in features_df['corridor_name'].unique():\n",
    "    corridor_data = features_df[features_df['corridor_name'] == corridor_name]\n",
    "    fraud_scores = corridor_data[corridor_data['is_fraud']]['amount_deviation']\n",
    "    ax2.hist(fraud_scores, bins=20, alpha=0.5, label=corridor_name, density=True)\n",
    "\n",
    "ax2.set_title('Corridor-Normalised Scores (Fraud Only)\\nMore consistent signal across corridors')\n",
    "ax2.set_xlabel('Score')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('global_vs_normalised.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Combined Feature Score\n",
    "\n",
    "For the next notebook, we'll prepare a simple combined score to use as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple equal-weighted combination (baseline)\n",
    "features_df['combined_score_baseline'] = (\n",
    "    features_df['amount_deviation'] * 0.25 +\n",
    "    features_df['velocity'] * 0.25 +\n",
    "    features_df['temporal_anomaly'] * 0.25 +\n",
    "    features_df['sender_maturity'] * 0.25\n",
    ")\n",
    "\n",
    "# Check performance\n",
    "combined_auc = roc_auc_score(features_df['is_fraud'], features_df['combined_score_baseline'])\n",
    "print(f'Combined score (equal weights) AUC: {combined_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features for next notebook\n",
    "features_df.to_csv('transaction_features.csv', index=False)\n",
    "print(f'Saved features to transaction_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Corridor profiling**: Building statistical baselines for each payment route\n",
    "\n",
    "2. **Normalised features**: Amount, velocity, temporal, and maturity scores calibrated to corridor norms\n",
    "\n",
    "3. **Quality improvement**: Corridor-normalised features provide better fraud signal than global features\n",
    "\n",
    "4. **Foundation for dynamic weighting**: Equal-weight combination is a baseline; next we'll optimise weights per corridor\n",
    "\n",
    "Next notebook: **Dynamic Signal Weighting** — learning optimal feature weights for each corridor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
